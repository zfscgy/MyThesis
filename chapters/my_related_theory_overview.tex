\chapter{相关理论综述}
为了统一描述，对于某个风控的数据集，我们把第j个用户的特征记为$\vec x_j = [x_{j,1}, x_{j,2}, ..., x_{j,i}]$, 因此第j个用户的第i个特征就是$x_{j,i}$。第j个用户的标签为$y_j$, 满足
\[
y_j =\begin{cases}  0 \quad \text{用户j的贷款按时偿还} \\ 1 \quad \text{用户j的贷款违约} \end{cases}
\]

我们希望可以找到函数$f(\vec x)$ 使得对于每个用户j，$f(\vec x_j)$ 都尽量接近 $y_j$， 因此可以用均方误差作为损失函数，即优化目标为 $\min \limits_{f} \sum_j (f(\vec x_j) - y_j)^2$
\section{模型综述}
\subsection{线性回归}
一般线性回归就是指最小二乘法拟合，最小化目标为
\[\min \limits_{w_i, b} \sum_{j} (\vec w \cdot \vec x_j + b - y_j)^2\]
 
很容易使用最小二乘法对$\vec w$和$b$进行估计。
\subsection{逻辑回归}
用均方误差作为损失函数的逻辑回归的目标为：
\[\min \limits_{w_i, b} \sum_{j} (\text{sigmoid} ( \vec w\cdot \vec x_j + b) - y_j)^2\]

其中，$ \text{sigmoid}(x) = \dfrac{1}{1+e^{-x}} $

一般使用梯度下降法求解参数。
\subsection{支持向量机}
支持向量机\cite{scholkopf2001svm}是一种强大的二分类算法，的最小化目标为：
\[ \min \limits_{w_i, b} \dfrac{1}{n} \sum_{j}(\max(0, 1-y_j(\vec w \cdot x_j))) + \lambda ||w||^2  \]

其原理就是找到一个超平面能够将$y=0$和$y=1$的两类样本尽可能分离开。

支持向量机可以通过求解对偶问题把原问题转换为带约束的二次规划问题，然后通过多种方法求解。同时支持向量机也支持Kernel Tricks，即通过定义Kernel Function(核函数)把原数据映射到无穷维内积空间，然后进行拟合。常用的有高斯核等。
\subsection{人工神经网络}
人工神经网络是一种从人类神经元连接方式启发而得到的机器学习模型，其每一层的基本方法是通过矩阵乘法将m维的数据映射到n维，然后再用激活函数产生输出，公式如下：
\[ \bm{y} = \text{activation}(W\bm{x} + \bm{b}) \]

神经网络可以有很多层，也可以有不同的结构，比如卷积神经网络、循环神经网络等。\cite{nndp}

神经网络具有强大的拟合能力，理论上足够大的单隐层神经网络可以逼近任意函数\cite{cybenko1989approximation}。
\subsection{决策树}
决策树\cite[311]{james2013introduction}是一种简单的模型，通过树结构来进行分类。决策树的每一个节点相当于一个if-else语句，对某个属性进行划分。对于该属性的值小于节点的值的样本，就把其送入左边的节点继续判断；反之则送入右边的节点。决策树把样本的向量空间分割为多个高维长方体，任何一个样本必然落在其中的一个长方体中。

决策树每一步从某一个叶节点出发，挑选一个属性进行分裂。一般来说分类决策树挑选分裂点的原则是信息增益，也就是让每个子节点的信息熵的和尽可能变小。也就是说其优化目标是：

\[ \min \sum_k \sum_m p_{mk}\log(p_{mk}) \]

其中， $p_{mk}$表示在叶子（区域）m上类别为k的样本的比例。

由于决策树的性质，理论上决策树可以对任何分布的样本进行分类。但是也很容易出现过拟合的现象，所以现在广泛采用大量浅层决策树的集成模型进行预测，如下面介绍的随机森林和XGBoost。

\subsection{随机森林}
随机森林\citet[p.~320]{james2013introduction}是多个决策树的集成模型，通过Boostrap采样算法从原始数据集中生成多个新的数据集，并分别用其训练不同的决策树模型。其中决策树选择分裂的时候仅仅从所有属性里面的一个子集挑选属性，以此降低过拟合的问题。
\subsection{XGBoost}
XGBoost\cite{chen2016xgboost}是另一种决策树的集成模型，可以用于回归和分类。对于某一样本，其输出就是XGBoost模型中的每一颗决策树对应的叶子节点的权重之和。XGBoost通过Boosting算法，在每轮迭代中加入一颗新的决策树来尽可能降低整体的损失函数。XGBoost通过限制单颗决策树的深度、部分采样以及对叶子和权重的规范化来减少过拟合。

XGBoost的算法通过预先索引、对属性的并行，部分采样等方式提高每棵树的生成速度，极大提高了运算效率。

\section{特征处理方法}

\subsection{主成分分析}
主成分分析\cite{shlens2014pca} 是一种降维的方法，把样本空间从m维降到k维(n<m)，满足
\[
\min \sum_{i = 1}^{n} ||\bm{x_i} - \sum_{j=1}^{k}(<\bm{x_i},\bm{\alpha_j}>\alpha_j)||^2
\]

即当样本从k维的“压缩状态”回到m维的时候，其损失(用差值的模平方衡量)之和最小。因此这是一种保留最多信息的线性映射。
\subsection{AutoEncoder}
AutoEncoder是一种输入和输出的大小相同的神经网络，其原理是把N维的输入数据转换到M维的隐藏层(M<N)，然后再转换成N维。AutoEncoder可以通过提取隐藏层来实现数据降维，也可以通过输出进行数据缺失值的填补。\cite{vincent2008autoencoder}

\section{模型效果评估方法}
\subsection{训练集和测试集}
为了评估模型的预测效果，我们需要从原始数据中划分一部分数据集作为测试集，测试集不参与训练。因为各种深度学习模型都具有强大的拟合能力，所以模型在训练集上的误差会趋向于0，所以为了评估模型真实的拟合能力，我们需要让训练好的模型在测试集上做出预测，查看其效果。
\subsection{K折交叉验证}
随着现代计算机计算性能的提升，对于大数据集的计算也不再昂贵。因此现在一般采用K折交叉验证的方式评估模型性能。即，把数据集分成K等份，依次选择其中的K-1份作为训练集，剩下的一份作为验证集，分别计算其误差。通过这种方式，可以显著降低评估误差，找出最佳的模型。\cite[213]{james2013introduction}
\subsection{模型评估指标}
\subsubsection{准确率}
模型的输出结果如果大于等于0.5，就认为其预测的标签是1，反之则为0。则准确率计算公式如下: 
\[
Accuracy = \dfrac{\text{预测准确的样本数}}{\text{总样本数}}
\]
\subsubsection{AUC(Area Under Roc Curve)}
ROC曲线，根据预测样本的真阳性率和假阳性率形成的曲线。其计算公式为
\[
    AUC = \int_{fpr=0}^1 tpr \mathrm{d}tpr
\]
其中，fpr表示假阳性率（实际标签为0，预测标签为1的样本数量/实际标签为0的样本总数（正样本总数））,tpr表示真阳性率（实际标签为1，预测标签为1的样本数量/实际标签为1的样本总数（负样本总数））
我们令预测标签为1的概率门槛从1变到0，则真阳性率和假阳性率将会从0变到1。设当有i个假阳性样本时，则假阳性率为i/N，N为所有实际标签为0的样本。不妨设第i的假阳性样本是刚刚加入的，也就是说模型对其的输出是这i个里面最小的，计其为$p_i$，则此时对应的真样本的数目为$\sum\limits_{j \in \text{正样本}} 1_{f(j) > f(i)}$
其中，
\[
1_{x>y} =\begin{cases}  0 \quad x<y \\ 1 \quad x>y \end{cases}
\]
根据以上的推导，可以看出AUC可以用如下公式计算：
\[
    AUC = \sum\limits_{i \in \text{正样本} } \sum\limits_{j \in \text{负样本} }{1_{f(i)>f(j)}}
\]
随机预测的AUC的值在0.5左右，AUC的值越大，说明模型越倾向于使得正样本的输出大于负样本的输出。