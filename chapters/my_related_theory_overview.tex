\chapter{相关理论综述}
本文所要完成的任务是根据用户的数据来预测用户的违约概率，因此是一个回归任务。下面介绍了几种最广泛使用的回归分析的模型，并且分析了其评估的效果和可行性，从中选择了三种模型作为本文主要采用的预测模型。
\section{问题描述}
我们的目标是给定一组用户的特征$ x \in R^m $，求出该用户的违约概率 $ y \in [0, 1]$。
为了统一描述，对于某个风控的数据集，我们把第j个用户的特征记为$\vec x_j = [x_{j,1}, x_{j,2}, ..., x_{j,n}]$, 因此第j个用户的第i个特征就是$x_{j,i}$。第j个用户的标签为$y_j$, 满足
\[
y_j =\begin{cases}  0 \quad \text{用户j的贷款按时偿还} \\ 1 \quad \text{用户j的贷款违约} \end{cases}
\]

我们希望可以找到函数$f(\vec x)$ 使得对于每个用户j，$f(\vec x_j)$ 都尽量接近 $y_j$， 因此可以用均方误差作为损失函数，即优化目标为 $\min \limits_{f} \sum_j (f(\vec x_j) - y_j)^2$。可以使用如下所述的多种回归模型对其拟合。

\section{回归模型}

\textbf{线性回归}:
线性回归也称作最小二乘法拟合，是最简单的回归方法，最小化目标为
\[\min \limits_{w_i, b} \sum_{j} (\vec w \cdot \vec x_j + b - y_j)^2\]
 
通过最小二乘法可以对$\vec w$和$b$进行估计。线性回归易于实现，并且具有良好的可解释性，拟合出来的$w_i$就是第i个特征的权重。

\textbf{逻辑回归}:
逻辑回归是一种常用的回归方法。用均方误差作为损失函数的逻辑回归的目标为：
\[\min \limits_{w_i, b} \sum_{j} (\text{sigmoid} ( \vec w\cdot \vec x_j + b) - y_j)^2\]

其中，$ \text{sigmoid}(x) = \dfrac{1}{1+e^{-x}} $。逻辑回归一般使用梯度下降法求解最佳参数。

逻辑回归和线性回归一样易于实现，并具有良好的可解释性。所不同的是逻辑回归拟合的值域是[0,1]，更加适合二分类数据。

\textbf{支持向量机}:
支持向量机\cite{scholkopf2001svm}是一种强大的二分类算法，最小化目标为：
\[ \min \limits_{w_i, b} \dfrac{1}{n} \sum_{j}(\max(0, 1-y_j(\vec w \cdot x_j))) + \lambda ||w||^2  \]

其原理就是找到一个超平面能够将$y=0$和$y=1$的两类样本尽可能分离开。

支持向量机可以通过求解对偶问题把原问题转换为带约束的二次规划问题，然后通过多种方法求解。同时支持向量机也支持Kernel Tricks，即通过定义Kernel Function(核函数)把原数据映射到无穷维内积空间，然后进行拟合。常用的有高斯核等。支持向量机具有非常强大的二分类功能。

\textbf{人工神经网络}:
人工神经网络是一种从人类神经元连接方式启发而得到的机器学习模型，其每一层的基本方法是通过矩阵乘法将m维的数据映射到n维，然后再用激活函数产生输出，公式如下：
\[ \bm{y} = \text{activation}(W\bm{x} + \bm{b}) \]

神经网络可以有很多层，也可以有不同的结构，比如卷积神经网络、循环神经网络等。\cite{nndp} 神经网络具有强大的拟合能力，理论上足够大的单隐层神经网络可以逼近任意函数\cite{cybenko1989approximation}。神经网络在各种回归和分类任务上都具有良好的表现，但是容易出现过拟合问题，并且缺少可解释性。

\textbf{决策树}:
决策树\cite[311]{james2013introduction}是一种简单的模型，通过树结构来进行分类。决策树的每一个节点相当于一个if-else语句，对某个属性进行划分。对于该属性的值小于节点的值的样本，就把其送入左边的节点继续判断；反之则送入右边的节点。决策树把样本的向量空间分割为多个高维长方体，任何一个样本必然落在其中的一个长方体中。

决策树每一步从某一个叶节点出发，挑选一个属性进行分裂。一般来说分类决策树挑选分裂点的原则是信息增益，也就是让每个子节点的信息熵的和尽可能变小。也就是说其优化目标是：

\[ \min \sum_k \sum_m p_{mk}\log(p_{mk}) \]

其中， $p_{mk}$表示在叶子（区域）m上类别为k的样本的比例。

由于决策树的性质，理论上决策树可以对任何分布的样本进行分类。但是也很容易出现过拟合的现象，所以现在广泛采用大量浅层决策树的集成模型进行预测，如下面介绍的随机森林和XGBoost。而且决策树具有非常好的可解释性，因此被广泛应用于各种特征无关的分类工作。

\textbf{随机森林}:
随机森林\cite[320]{james2013introduction}是多个决策树的集成模型，通过Boostrap采样算法从原始数据集中生成多个新的数据集，并分别用其训练不同的决策树模型。其中决策树选择分裂的时候仅仅从所有属性里面的一个子集挑选属性，以此降低过拟合的问题。
随机森林能够显著提高决策树的效果，并且保留了决策树的可解释性。

\textbf{XGBoost}:
XGBoost\cite{chen2016xgboost}是另一种决策树的集成模型，可以用于回归和分类。对于某一样本，其输出就是XGBoost模型中的每一棵决策树对应的叶子节点的权重之和。XGBoost通过Boosting算法，在每轮迭代中加入一棵新的决策树来尽可能降低整体的损失函数。XGBoost通过限制单棵决策树的深度、部分采样以及对叶子和权重的规范化来减少过拟合。

XGBoost的算法通过预先索引、对属性的并行，部分采样等方式提高每棵树的生成速度，极大提高了运算效率。
\\

因为我们的问题是对用户是否违约进行预测，是一个二分类问题，因此直接使用线性回归的效果较差。支持向量机的运算开销过大，需要十几个小时才能做出一组预测，因此在之后的实验中没有采用该方法。同时决策树、随机森林和Xgboost呈现递进关系，Xgboost的一轮迭代就是决策树。因此在后文中我们采取了其中逻辑回归、人工神经网络和Xgboost这三个模型进行实验。

\section{特征处理方法}
如果原始数据的维度过高，或是存在一些缺失值，会影响模型的拟合效果，或是增加拟合的成本。一般可以通过主成分分析解决原始数据维度过高的问题，通过AutoEncoder去除原始数据中的缺失值、噪声等异常数据。

\textbf{主成分分析}:
主成分分析\cite{shlens2014pca} 是一种降维的方法，把样本空间从m维降到k维(k<m)，满足
\[
\min \sum_{i = 1}^{n} ||\bm{x_i} - \sum_{j=1}^{k}(<\bm{x_i},\bm{\alpha_j}>\alpha_j)||^2
\]

即当样本从k维的“压缩状态”回到m维的时候，其损失(用差值的模平方衡量)之和最小。因此这是一种保留最多信息的线性映射。

\textbf{AutoEncoder}:
AutoEncoder是一种输入和输出的大小相同的神经网络，其原理是把N维的输入数据转换到m维的隐藏层(m<n)，然后再转换成N维。AutoEncoder可以通过提取隐藏层来实现数据降维，也可以通过再设置一个与输入同样大小的输出层，对输入数据进行数据缺失值的填补。\cite{vincent2008autoencoder}

因为现在深度学习的任务都可以在GPU上运行，因此可以接受维度很高的数据，而主成分分析必然损失一部分信息。因此在之后的讨论中我们均使用原始数据，并未采用主成分分析对其进行降维。

\section{模型效果评估方法}
\subsection{训练集和测试集}
为了评估模型的预测效果，我们需要从原始数据中划分一部分数据集作为测试集，测试集不参与训练。因为各种深度学习模型都具有强大的拟合能力，所以模型在训练集上的误差会趋向于0，所以为了评估模型真实的拟合能力，我们需要让训练好的模型在测试集上做出预测，查看其效果。
\subsection{K折交叉验证}
随着现代计算机计算性能的提升，对于大数据集的计算也不再昂贵。因此现在一般采用K折交叉验证的方式评估模型性能。即，把数据集分成K等份，依次选择其中的K-1份作为训练集，剩下的一份作为验证集，分别计算其误差。通过这种方式，可以显著降低评估误差，降低某些偶然因素导致的评估的不准确性，找出最佳的模型。\cite[213]{james2013introduction}
\subsection{模型评估指标}
\subsubsection{准确率}
模型的输出结果如果大于等于0.5，就认为其预测的标签是1，反之则为0。则准确率计算公式如下: 
\[
Accuracy = \dfrac{\text{预测准确的样本数}}{\text{总样本数}}
\]
\subsubsection{AUC(Area Under Roc Curve)}
ROC曲线，根据预测样本的真阳性率和假阳性率形成的曲线。其计算公式为
\[
    AUC = \int_{fpr=0}^1 tpr \mathrm{d}tpr
\]
其中，fpr表示假阳性率（实际标签为0，预测标签为1的样本数量/实际标签为0的样本总数（正样本总数））,tpr表示真阳性率（实际标签为1，预测标签为1的样本数量/实际标签为1的样本总数（负样本总数））
我们令预测标签为1的概率门槛从1变到0，则真阳性率和假阳性率将会从0变到1。设当有i个假阳性样本时，则假阳性率为i/N，N为所有实际标签为0的样本。不妨设第i的假阳性样本是刚刚加入的，也就是说模型对其的输出是这i个里面最小的，计其为$p_i$，则此时对应的真样本的数目为$\sum\limits_{j \in \text{正样本}} 1_{f(j) > f(i)}$
其中，
\[
1_{x>y} =\begin{cases}  0 \quad x<y \\ 1 \quad x>y \end{cases}
\]
根据以上的推导，可以看出AUC可以用如下公式计算：
\[
    AUC = \sum\limits_{i \in \text{正样本} } \sum\limits_{j \in \text{负样本} }{1_{f(i)>f(j)}}
\]
随机预测的AUC的值在0.5左右，AUC的值越大，说明模型越倾向于使得正样本的输出大于负样本的输出。