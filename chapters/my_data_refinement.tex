\chapter{风控原始数据预处理}
本文中所使用的数据集来自某互联网公司的数个风控平台的数据库。因为原始数据的格式不统一，含有大量字符串、压缩数据、列表、JSON等数据，因此必须经过处理才可以作为模型输入。数据预处理的好坏会极大影响模型预测的效果\cite{kotsiantis2006data}。我们的处理目标就是把原始数据中可能有用的数据尽可能提取成数值，同时尽可能丢弃无效的数据，保留对模型预测有效的数据，返回一个 (用户数目$\times$单个用户特征数目大小) 的表。并且表中的每一项数值都介于0到1之间，有利于对于数值敏感的模型的训练。
\section{大数据平台Hadoop}
Hadoop\footnote{https://hadoop.apache.org}是一个开源的大数据平台，提供了一套分布式存储和分布式任务调度的框架，能够让服务器集群更加有效地处理超大规模的数据。Spark\footnote{https://spark.apache.org/docs/latest/}是一套快速通用的集群计算系统，能够执行大量数据处理的任务，并且提供了Python、Java和Scala语言的高级API。Spark可以通过Hadoop进行任务调度。
\section{数据格式统一}
\subsection{JSON数据的提取}
JSON\footnote{https://www.json.org}是一种轻量级的数据表示格式，可以表示key-value类型的对象，以及列表，支持嵌套。每个对象的值可以是对象、字符串、数字或者是布尔值以及null。下列代码是一个简单的JSON数据示例。
\begin{lstlisting}[language=PYTHON, caption={JSON示例}, label={JSON示例}]
{
	"姓名":"张三",
	"家属": 
	{
		"父亲":"张父",
		"母亲":"张母"
	},
	"出生日期":"1999-9-9",
	"学历记录":
	[
		{
			"中学":"张村中学",
			"成绩":"95"
		},
		{
			"大学":"中国科学技术大学",
			"成绩":"1.0"
		}
	]
}
\end{lstlisting}

对于JSON格式的数据，处理方式为:

递归地取出所有非列表结构的字段。为了在数据处理之后不丢失原有的属性的信息，按照属性名拼接的方式产生列名。比如对于上面的示例中的“父亲”一项，就把值保存在列名为"家属\_\_\_父亲"的列中。对于列表形式的数据，根据实际情况，一般列表当中的对象都是字典类型，所以把其中所有数值类型字段进行求和和平均，加入列明为属性名拼接加上“\_\_\_SUM”或"\_\_\_AVG"标识符的列中，比如上面示例里的“学历记录”可以产生两列名为“学历记录\_\_\_成绩\_\_\_SUM”和“学历记录\_\_\_成绩\_\_\_AVG”的记录。

\begin{algorithm}[htbp]
	\SetAlgoLined
	\KwData{Json}
	\KwResult{OutputDict}
	OutputDict = \{\}\\
	UnParsedJsonElems = [Json]\\
	Prefix = ""\\
	\While{UnParsedJsonElems is not empty}{
		elem = UnParsedJsonElems.pop(0)\\
		\For{attr in elem.stringAndNumericalAttributes}
		{
			OutputDict[Prefix + attr] = elem[attr]\\
		}
		\For{attr in elem.listAttributes}
		{
			\For{subAttr in elem[attr][0].numericalAttributes}
			{
				OutputDict[Prefix+"\_\_\_"+attr + "\_\_\" + subAttr + "\_\_\_SUM"] = SUM(elem[attr], subAttr) \\
				OutputDict[Prefix+"\_\_\_"+attr + "\_\_\" + subAttr + "\_\_\_AVG"] = AVG(elem[attr], subAttr) \\
			}
		}
		\For{attr in elem.objectAttributes}
		{
			UnparsedJsonElem.push(elem[attr])
		}
	}
	\caption{Json处理算法示例}
	\label{algo:algorithm1}
\end{algorithm}
\subsection{特大数量异构JSON数据融合}
由于实验的数据集中含有数十万条用户的JSON数据，而且通过样本得知每个样本的JSON数据都略有不同，比如用户2可能有用户1中不存在的字段，用户1也可能有用户2中不存在的字段。由于整体数据量过于庞大，如果要遍历所有的用户数据，由于各用户JSON字段的差别，无法并行地进行处理，因此效率极其低下。为了获取大部分用户都拥有的字段，我们采用采样的方法。

假设某一字段a, 在数据集的所有用户中出现的频率是p，那么缺失率就是1-p。在N次采样中，字段a都没有出现的概率为
\[ p_{\text{miss in N samples}} = (1-p)^N\]

因此如果取采样数目为80，那么对于在样本中出现频率仅为10\%的字段，其在采样中不出现的概率也仅仅不到$\frac{1}{5000}$，因此我们可以认为大小为80的采样样本中包含了几乎所有出现频率大于0.1的字段。

因此首先把采样中出现的字段存放在字典中，将每个字段对应到一个整数，可以大大减少因为字段过长产生的冗余内存。然后通过该字典把用户的JSON数据进行统一处理，如果用户含有不包含在字典中的JSON字段则丢弃，如果用户缺少字典中的JSON字段，则将该字段补为空值。同时把字段名都改为字典中对应字段的整数Value。

\subsection{多源数据整合}
数据集中的每个表都有一个唯一的用户ID标识，因此可以通过表连接的方式获得总表。
\subsection{数据的数值化转换}
由于不同的数据源使用不同的方法表示数据的缺失、异常值，比如有的采用字符串"无数据"表示，有的使用null值表示，有的使用空字符串表示，所以转换的时候需要注意，并且将其都转换为nan值。

为了获取每一列可能的非数值数据，通过遍历的方法，逐列进行检查，把每一列出现的非数值值记录到文件里并且人工检查之。尽可能把能够量化的字符串变量进行数值化转换。如对于“用户与律师通话频率”字段，有“频繁通话”、“正常通话”、“很少通话”、“从未通话”四种字符串，可以分别转换为1、0.667、0.333、0四种数值。

对于类别类型的数据，比如“省份”之类，采用one-hot编码，也就是说把列数从原来的一列扩充到与类别数量相同的列数。



\section{数据清洗}
由于通过前面步骤得到的数据量依然非常巨大，而大量对模型预测无效的数据会对模型产生不良影响，同时数据量过大也会极大地增加模型的训练和运行成本，所以我们需要对每一列数据进行初步分析，删除那些可能无效的列。

\subsection{数据的初步分析}
我们采用的方法是对每一列数据进行分析。
\begin{itemize}
	\item 缺失率：即该列数据在所有样本中缺失的比率。如果缺失率过高，则考虑丢弃该列。
	\item 最大值、最小值、标准差。这些数值可以反映该列数据的变动程度。如果最大值非常接近最小值或者标准差过小，则可认为该列数据在每一个样本上变动很小，没有反映出样本之间的差异性，考虑丢弃该列。
	\item 与标签的相关系数。如果与标签的相关系数很小，则说明该列很可能没有有用信息。但是也有可能存在该列数据仅仅和标签的一阶相关系数小，可能有其他的非线性关系或者和别的特征组合影响标签的情况。
\end{itemize}
我们通过以上三点综合考虑每一列数据的有效性，最终决定是否要丢弃该列数据。

为了使得最后的数据分布在0—1之间，把每一列都按照如下方式处理：

$normalized\_value = \dfrac{value - min}{max - min}$