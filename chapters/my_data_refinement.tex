\chapter{风控原始数据预处理}\label{data_refinement}
本文中所使用的数据集来自某互联网公司的数个风控平台的数据库。用户是否违约的标签来自某互联网公司自己的数据库。因为原始数据的格式不统一，含有大量字符串、压缩数据、列表、JSON等数据，因此必须经过处理才可以作为模型输入。数据预处理的好坏会极大影响模型预测的效果\cite{kotsiantis2006data}。我们的处理目标就是把原始数据中可能有用的数据尽可能提取成数值，同时尽可能丢弃无效的数据，保留对模型预测有效的数据，返回一个 (用户数目$\times$单个用户特征数目大小) 的表。并且表中的每一项数值都介于0到1之间，有利于对于数值敏感的模型的训练。
\section{大数据平台Hadoop}
Hadoop\footnote{https://hadoop.apache.org}是一个开源的大数据平台，提供了一套分布式存储和分布式任务调度的框架，能够让服务器集群更加有效地处理超大规模的数据。Spark\footnote{https://spark.apache.org/docs/latest/}是一套快速通用的集群计算系统，能够执行大量数据处理的任务，并且提供了Python、Java和Scala语言的高级API。Spark可以通过Hadoop进行任务调度。

我们的数据处理任务，主要需要使用Hadoop的HIVE分布式数据库，并且使用Spark的Python API进行访问。原始数据以HIVE数据库的形式存储。
\section{数据格式统一}
\subsection{JSON数据的提取}
JSON\footnote{https://www.json.org}是一种轻量级的数据表示格式，可以表示key-value类型的对象，以及列表，支持嵌套。每个对象的值可以是对象、字符串、数字或者是布尔值以及null。下列代码是一个简单的JSON数据示例。
\begin{lstlisting}[language=PYTHON, caption={JSON示例}, label={JSON示例}]
{
	"姓名":"张三",
	"家属": 
	{
		"父亲":"张父",
		"母亲":"张母"
	},
	"出生日期":"1999-9-9",
	"学历记录":
	[
		{
			"中学":"张村中学",
			"成绩":"95"
		},
		{
			"大学":"中国科学技术大学",
			"成绩":"1.0"
		}
	]
}
\end{lstlisting}

对于JSON格式的数据，处理方式为:

递归地取出所有非列表结构的字段。为了在数据处理之后不丢失原有的属性的信息，按照属性名拼接的方式产生列名。比如对于上面的示例中的“父亲”一项，就把值保存在列名为"家属\_\_\_父亲"的列中。对于列表形式的数据，首先把列表的长度作为特征提取出来，产生列名为属性名加上“\_\_\_LEN”的列。然后根据实际情况，一般列表当中的对象都是字典类型，所以把其中所有数值类型字段进行求和和平均，加入列名为属性名拼接加上“\_\_\_SUM”或"\_\_\_AVG"标识符的列中，比如上面示例里的“学历记录”可以产生三列记录，列名分别为“学历记录\_\_\_LEN”、“学历记录\_\_\_成绩\_\_\_SUM”和“学历记录\_\_\_成绩\_\_\_AVG”。具体算法参见算法\ref{algo:json}。

\begin{algorithm}[htbp]
	\SetAlgoLined
	\KwData{Json}
	\KwResult{OutputDict}
	OutputDict = \{\}\\
	UnParsedJsonElems = [Json]\\
	Prefix = ""\\
	\While{UnParsedJsonElems is not empty}{
		elem = UnParsedJsonElems.pop(0)\\
		\For{attr in elem.stringAndNumericalAttributes}
		{
			OutputDict[Prefix + attr] = elem[attr]\\
		}
		\For{attr in elem.listAttributes}
		{
			\For{subAttr in elem[attr][0].numericalAttributes}
			{
				OutputDict[Prefix+"\_\_\_"+attr + "\_\_\" + subAttr + "\_\_\_SUM"] = SUM(elem[attr], subAttr) \\
				OutputDict[Prefix+"\_\_\_"+attr + "\_\_\" + subAttr + "\_\_\_AVG"] = AVG(elem[attr], subAttr) \\
			}
		}
		\For{attr in elem.objectAttributes}
		{
			UnparsedJsonElem.push(elem[attr])
		}
	}
	\caption{Json处理算法示例}
	\label{algo:json}
\end{algorithm}
\subsection{异构JSON数据融合}
由于实验的数据集中含有数十万条用户的JSON数据，而且通过样本得知每个样本的JSON数据都略有不同，比如用户2可能有用户1中不存在的字段，用户1也可能有用户2中不存在的字段。由于整体数据量过于庞大，如果要遍历所有的用户数据，由于各用户JSON字段的差别，无法并行地进行处理，因此效率极其低下。为了获取大部分用户都拥有的字段，我们采用采样的方法。

假设某一字段a, 在数据集的所有用户中出现的频率是p，那么缺失率就是1-p。在N次采样中，字段a都没有出现的概率为
\[ p_{\text{miss in N samples}} = (1-p)^N\]

因此如果取采样数目为80，那么对于在样本中出现频率仅为10\%的字段，其在采样中不出现的概率也仅仅不到$\frac{1}{5000}$，因此我们可以认为大小为80的采样样本中包含了几乎所有出现频率大于0.1的字段。

因此首先把采样中出现的字段存放在字典中，将每个字段对应到一个整数，可以大大减少因为字段过长产生的冗余内存。然后通过该字典把用户的JSON数据进行统一处理，如果用户含有不包含在字典中的JSON字段则丢弃，如果用户缺少字典中的JSON字段，则将该字段补为空值。同时把字段名都改为字典中对应字段的整数值。

\subsection{多源数据整合}
数据集中的每个表都有一个唯一的用户ID标识，因此可以通过表连接的方式获得总表。也就是说，最后的总表包含的用户是每个表包含的用户的交集。对于我们所使用的风控数据集，最后的总表的用户数目为20多万。

\subsection{数据的数值化转换}
由于不同的数据源使用不同的方法表示数据的缺失、异常值，比如有的采用字符串"无数据"表示，有的使用null值表示，有的使用空字符串表示，所以转换的时候需要注意，并且将其都转换为浮点数的nan值。

为了获取每一列可能的非数值数据，通过遍历的方法，逐列进行检查，把每一列出现的非数值值记录到文件里并且人工检查之。尽可能把能够量化的字符串变量进行数值化转换。如对于“用户与律师通话频率”字段，有“频繁通话”、“正常通话”、“很少通话”、“从未通话”四种字符串，可以分别转换为1、0.667、0.333、0四种数值。

对于类别类型的数据，一般对于类别数目较多的情况，则用该类别的标签平均值代替。如果类别较少，则直接进行one-hot编码。

\section{数据清洗}
由于通过前面步骤得到的数据量依然非常巨大，而大量对模型预测无效的数据会对模型产生不良影响，同时数据量过大也会极大地增加模型的训练和运行成本，所以我们需要对每一列数据进行初步分析，删除那些可能无效的列。

我们首先对每一列的数据进行分析。
\begin{itemize}
	\item 缺失率：即该列数据在所有样本中缺失的比率。如果缺失率过高，则考虑丢弃该列。
	\item 最大值、最小值、标准差。这些数值可以反映该列数据的变动程度。如果最大值非常接近最小值或者标准差过小，则可认为该列数据在每一个样本上变动很小，没有反映出样本之间的差异性，考虑丢弃该列。
	\item 与标签的相关系数。如果与标签的相关系数很小，则说明该列很可能没有有用信息。但是也有可能存在该列数据仅仅和标签的一阶相关系数小，可能有其他的非线性关系或者和别的特征组合影响标签的情况。
\end{itemize}
我们通过以上三点综合考虑每一列数据的有效性，最终决定是否要丢弃该列数据。

因为很多模型对于数据的范围敏感，比如神经网络的sigmoid激活函数在数据的绝对值过大时就趋于平缓。同时过大的数据还有可能产生溢出或数据转换出错等问题，因此我们把最后的数据转换为在0—1之间，把每一列都按照如下方式处理：
\[normalized\_value = \dfrac{value - min}{max - min}\]

\section{数据导出}
最后为了把数据从Hadoop服务器中取出，我们调用了Pyspark Dataframe的collect方法，直接把数据调用到Spark Driver的内存中。由于内存有限，所以一次只能取出10万条用户记录。我们把这10万条用户记录以Numpy二维数组的方式存储，并且把其格式转换为32位浮点数，然后通过numpy.save保存到Spark所在的Slave机的硬盘上。这样子能比使用CSV字符化存储节省约70\%的空间。